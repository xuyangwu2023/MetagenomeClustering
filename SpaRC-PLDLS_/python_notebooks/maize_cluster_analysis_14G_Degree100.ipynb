{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the results of overlapped community resolution by Label Propagation Analysis (LPA)\n",
    "##通过标签传播分析（LPA）检查重叠社区解析的结果\n",
    "Hw_huawei@123, such as: ssh spark@160.44.194.183\n",
    "/home/spark/localcluster/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, gzip\n",
    "from cffi.backend_ctypes import xrange\n",
    "\n",
    "add_libs = [\n",
    "\"/global/u1/z/zhwang/zhong/lib/python2.7/site-packages\",\n",
    "\"/global/u1/z/zhwang/zhong/lib/python2.7/site-packages/IPython/extensions\"\n",
    "]\n",
    "[sys.path.append(l) for l in add_libs]\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "##设置图像细节\n",
    "sns.set()\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotated_key(keyfile, inputfile):\n",
    "    '''\n",
    "    Label each read with annotated transcript id \n",
    "    If not labelled, use \"na\"\n",
    "    return a dictionary seqID -> transcript_id\n",
    "    '''\n",
    "    ## read annotated transcripts\n",
    "    keyinfo = {}\n",
    "    if not(os.path.exists(keyfile) and os.path.exists(inputfile)):\n",
    "        print (\"at least one of the input files are not found.\")\n",
    "        sys.exit(0)\n",
    "    with open(keyfile, 'r') as KEY:\n",
    "        for lines in KEY:#.readlines():\n",
    "            try:\n",
    "                anarray = lines.strip(\"\\n\").split(\"\\t\")\n",
    "                keyinfo[anarray[0]] = anarray[1]\n",
    "            except:\n",
    "                continue\n",
    "    KEY.close()\n",
    "    #将keyinfo中的view对象长度填入{}中\n",
    "    print (\"Number of reads in the annotation: \" + '{:,d}'.format(len(keyinfo.keys())))  #keys:keys() 方法返回 view 对象。这个视图对象包含列表形式的字典键。\n",
    "    known_clusters = pd.DataFrame.from_dict(keyinfo, 'index')\n",
    "    known_clusters = known_clusters.rename(columns = {0:'transcript'})#将第0列的列名改为transcript\n",
    "    known_counts = known_clusters.groupby(['transcript']).size()\n",
    "    print (\"Total reads in annotated clusters: \" + '{:,d}'.format(known_counts[known_counts>1].sum()))\n",
    "    print (\"Total annotated clusters: \" + '{:,d}'.format(sum(known_counts>1)))\n",
    "    print (\"The top 10 biggest annotated clusters are:\")\n",
    "    print (known_counts.sort_values(ascending=False)[0:10])\n",
    "    \n",
    "    ## annotate input reads\n",
    "    annotations = {}\n",
    "    no_input_reads = 0\n",
    "    no_annotated_reads = 0\n",
    "    with open(inputfile, 'r') as IN:\n",
    "        for lines in IN:#.readlines():\n",
    "            seq_id,seq = lines.strip(\"\\n\").split(\"\\t\")\n",
    "            header = seq_id.split(\" \")[0]\n",
    "            try:\n",
    "                tid = keyinfo[header]\n",
    "                no_annotated_reads += 1\n",
    "            except:\n",
    "                tid = 'na'\n",
    "            annotations[seq_id] = tid    \n",
    "            no_input_reads +=1\n",
    "    print (\"Number of reads in the input: \" + '{:,d}'.format(no_input_reads))\n",
    "    print (\"Number of reads annotated in the input: \" + '{:,d}'.format(no_annotated_reads))\n",
    "    return annotations\n",
    "\n",
    "def annotate_clusters(cluster, annotations):\n",
    "    ''' \n",
    "    parse spark cluster results\n",
    "    cluster file format is: seqid \\t cluster_id\n",
    "    return: [seq_name, cluster_id, annotation_transcript]\n",
    "    \n",
    "    ''' \n",
    "    if not os.path.exists(cluster):\n",
    "        print (\"Cluster file not found.\")\n",
    "        sys.exit(0)\n",
    "        \n",
    "    results = []\n",
    "    total_clustered_reads = 0\n",
    "    total_clustered_annotated = 0\n",
    "    with gzip.open(cluster, 'r') as IN:\n",
    "        for lines in IN:#.readlines():\n",
    "            seq_id, group_id = lines.strip(\"\\n\").split(\"\\t\")\n",
    "            seq_id=seq_id.split('/')[0]\n",
    "            total_clustered_reads += 1\n",
    "            if group_id > 0 and annotations[seq_id] != 'na':\n",
    "                results.append([seq_id, group_id, annotations[seq_id]])\n",
    "                total_clustered_annotated += 1\n",
    "                \n",
    "    IN.close()\n",
    "    print (\"Total reads in clusters: \" + '{:,d}'.format(total_clustered_reads))\n",
    "    print (\"Total annotated reads in clusters: \" + '{:,d}'.format(total_clustered_annotated))\n",
    "\n",
    "    return pd.DataFrame(results) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nmi(config, min_cluster_size):\n",
    "    cluster_ids = set(config[1])\n",
    "    data = []\n",
    "    for name,group in config.groupby([2]):\n",
    "        if len(group)<min_cluster_size:\n",
    "            continue\n",
    "        d = {c:0 for c in cluster_ids}    \n",
    "        for g in group[1]:\n",
    "            d[g] +=1\n",
    "        d.pop('-1', None) # remove unclustered reads   \n",
    "        data.append([d[k] for k in sorted(d.keys())]) \n",
    "    data = np.array(data).astype(float) \n",
    "    tc = np.copy(data)\n",
    "    tcp = tc/np.sum(tc)\n",
    "    ws = [ np.sum(tc[i])/(np.sum(tc)+0.0000000000001) for i in xrange(len(tc))]\n",
    "    cs = [ np.sum(np.transpose(tc)[i])/(np.sum(tc)+0.0000000000001) for i in xrange(len(np.transpose(tc)))]\n",
    "    if len(ws)>1 and len(cs)>1:\n",
    "        II = 0\n",
    "        for i in xrange(len(ws)):\n",
    "            for j in xrange(len(cs)):\n",
    "                II += tcp[i][j] * np.log((tcp[i][j])/(cs[j]*ws[i]+0.0000000000001) + 0.000000000001)\n",
    "        Hc = np.sum([ -csi * np.log(csi + 0.000000000001) for csi in cs])\n",
    "        Hw = np.sum([ -cwi * np.log(cwi + 0.000000000001) for cwi in ws])\n",
    "        H = (Hc+Hw)*0.5\n",
    "        NMI = II/H\n",
    "        return NMI,II\n",
    "    else:\n",
    "        return 'len 1'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_stat(config, figure_name):\n",
    "    '''\n",
    "    [seq_name, cluster_id, annotation_transcript]\n",
    "    '''\n",
    "    ## purity\n",
    "    grouped = config.groupby([1])\n",
    "    cluster_results = []\n",
    "    for name,group in grouped:\n",
    "        if len(group)<2:\n",
    "            continue\n",
    "        counts = group.groupby([2]).count()[0]\n",
    "        cluster_results.append([ len(counts), sum(counts), max(counts), counts[counts == max(counts)].index[0]])\n",
    "    cluster_results = pd.DataFrame(cluster_results)\n",
    "    clustered_reads = sum(cluster_results[1])\n",
    "    print (\"Total clusters from Input: \" + '{:,d}'.format(cluster_results.shape[0]))\n",
    "    print ('{:,d}'.format(clustered_reads + \" reads are in clusters\"))\n",
    "    top10 = cluster_results.sort_values([1], ascending=[0])[0:10]\n",
    "    \n",
    "    print (\"The largest 10 clusters are:\")\n",
    "    print (\"#transcripts\\treads\\tpurity\\tID\")\n",
    "    for c in top10.iterrows():  \n",
    "        c1, c2, c3, c4 = c[1]\n",
    "        print (\"%d\\t\\t%d\\t%.2f\\t%s\" % (c1, c2, 100.0*c3/c2, c4))\n",
    "    print (\"Percent of 100% pure clusters: \" + '{:.2f}'.format(100.0* sum(cluster_results[0] == 1)/cluster_results.shape[0]))\n",
    "    print (\"Median purity: \" + '{:.2f}'.format(np.median(100.0* cluster_results[2] / cluster_results[1])))\n",
    "    #print \"Mean purity: \" + '{:.2f}'.format(np.mean(100.0* cluster_results[:,2] / cluster_results[:,1]))\n",
    "#    nmi, ii = get_nmi(config, 2) \n",
    "#    print \"Normalized Mutual Information: \" + '{:.2f}'.format(nmi)\n",
    "    # completeness\n",
    "    grouped = config.groupby([2])\n",
    "    completeness = []\n",
    "    for name,group in grouped:\n",
    "        if len(group) <= 2:\n",
    "            continue\n",
    "        counts = group.groupby([1]).count()[0]\n",
    "        completeness.append([max(counts), sum(counts)])\n",
    "    completeness = np.array(completeness)    \n",
    "    median_completeness = np.median(100.0 * completeness[:,0] / completeness[:,1])\n",
    "    mean_completeness = np.mean(100.0 * completeness[:,0] / completeness[:,1])\n",
    "    \n",
    "    print (\"The most abundant transcript has \" + '{:,d}'.format(max(completeness[:,1]))  + \" copies.\")\n",
    "    print (\"Median completeness: \" + '{:.2f}'.format(median_completeness))\n",
    "    #print \"Mean completeness: \" + '{:.2f}'.format(mean_completeness)    \n",
    "    \n",
    "    fig1 = plt.figure(num=1, figsize=(12, 12), dpi=300)\n",
    "    plt.subplots_adjust( wspace=.1, hspace=.1 )\n",
    "\n",
    "    ax1 = fig1.add_subplot(3,3,1)\n",
    "    ax1.set_title('Clusters Size (log10) vs purity')\n",
    "    ax1.set_ylim(top=102)\n",
    "    x = np.log10(cluster_results[1])\n",
    "    y = 100.0* cluster_results[2] / cluster_results[1]\n",
    "    sns.kdeplot(x, y,shade=True, shade_lowest=False, n_levels=100, ax=ax1);\n",
    "\n",
    "    ax2 = fig1.add_subplot(3,3,2)\n",
    "    ax2.set_title('#Transcripts(log2) per Cluster')\n",
    "    sns.distplot(np.log2(cluster_results[0]), kde=False, bins=100, ax=ax2)\n",
    "    \n",
    "    ax3 = fig1.add_subplot(3,3,3)\n",
    "    ax3.set_title('#Transcripts (log10) vs completeness')\n",
    "    ax3.set_ylim(top=102)\n",
    "    x = np.log10(completeness[:,1])\n",
    "    y = 100.0 * completeness[:,0] / completeness[:,1]\n",
    "    sns.kdeplot(x, y,shade=True, shade_lowest=False, n_levels=100, ax=ax3);\n",
    "\n",
    "    # large clusters\n",
    "    large = 1\n",
    "    x = np.log10(cluster_results[1])\n",
    "    sel = x>=large\n",
    "    if np.sum(sel)>10:\n",
    "        ax4 = fig1.add_subplot(3,3,4)\n",
    "        ax4.set_title('Clusters Size(log10) vs purity')\n",
    "        y = 100.0* cluster_results[2] / cluster_results[1]\n",
    "        sns.kdeplot(x[sel], y[sel], n_levels=10, shade=True, shade_lowest=False, ax=ax4);\n",
    "        ax4.set_xlim(left=0)\n",
    "        ax4.set_ylim(top=102)\n",
    "\n",
    "        ax5 = fig1.add_subplot(3,3,5)\n",
    "        ax5.set_title('#Transcripts(log2) per Cluster')\n",
    "        ax5.set_ylim(top=2000)\n",
    "        sns.distplot(np.log2(cluster_results[0]), kde=False, ax=ax5)\n",
    "\n",
    "        ax6 = fig1.add_subplot(3,3,6)\n",
    "        ax6.set_title('#Transcripts (log10) vs completeness')\n",
    "        x = np.log10(completeness[:,1])\n",
    "        sel = x>=large\n",
    "        y = 100.0 * completeness[:,0] / completeness[:,1]\n",
    "        sns.kdeplot(x[sel], y[sel], n_levels=10, shade=True, shade_lowest=False, ax=ax6);\n",
    "        ax6.set_xlim(left=0)\n",
    "        ax6.set_ylim(top=102)\n",
    " \n",
    "    # largest clusters\n",
    "    large = 2\n",
    "    x = np.log10(cluster_results[1])\n",
    "    sel = x>=large \n",
    "    if np.sum(sel)>1:\n",
    "        ax7 = fig1.add_subplot(3,3,7)\n",
    "        ax7.set_title('Clusters Size(log10) vs purity')\n",
    "        y = 100.0* cluster_results[2] / cluster_results[1]\n",
    "        sns.regplot(x=x[sel], y=y[sel], ax=ax7);\n",
    "        ax7.set_xlim(left=0)\n",
    "        ax7.set_ylim(top=102)\n",
    "\n",
    "        ax8 = fig1.add_subplot(3,3,8)\n",
    "        ax8.set_title('#Transcripts(log2) per Cluster')\n",
    "        ax8.set_ylim(top=100)\n",
    "        sns.distplot(np.log2(cluster_results[0]), kde=False, ax=ax8)\n",
    "\n",
    "        ax9 = fig1.add_subplot(3,3,9)\n",
    "        ax9.set_title('#Transcripts (log10) vs completeness')\n",
    "        x = np.log10(completeness[:,1])\n",
    "        sel = x>=large\n",
    "        y = 100.0 * completeness[:,0] / completeness[:,1]\n",
    "        sns.regplot(x=x[sel], y=y[sel], ax=ax9);\n",
    "        ax9.set_xlim(left=0)\n",
    "        ax9.set_ylim(top=102)\n",
    "    \n",
    "    sns.plt.tight_layout()\n",
    "    sns.plt.show()\n",
    "    fig1.savefig(figure_name + '.pdf', format='pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reads in the annotation: 994,512\n",
      "Total reads in annotated clusters: 994,512\n",
      "Total annotated clusters: 33,853\n",
      "The top 10 biggest annotated clusters are:\n",
      "transcript\n",
      "14327    6730\n",
      "18989    6213\n",
      "2785     4849\n",
      "10122    4121\n",
      "4876     3964\n",
      "17174    3378\n",
      "15981    3208\n",
      "22792    3192\n",
      "14029    2772\n",
      "25598    2246\n",
      "dtype: int64\n",
      "Number of reads in the input: 2,183,954\n",
      "Number of reads annotated in the input: 0\n"
     ]
    }
   ],
   "source": [
    "## inputfile = '/home/spark/data/raw/maize/1557.4.notcombinednew.seq'\n",
    "## keyfile = \"/home/spark/localcluster/tmp/1557.4.notcombined.mapping.filtered.csv\"\n",
    "inputfile = 'D:\\\\Study\\\\SAPRC\\\\temp\\\\north\\\\north_Sea1.seq'   ##无序号的seq文件\n",
    "keyfile = \"D:\\\\Study\\\\SAPRC\\\\temp\\\\north\\\\addseq_northseal.txt\"     #SeqAddid输出文件\n",
    "keyinfo = get_annotated_key(keyfile, inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'D:\\\\Study\\\\SAPRC\\\\temp\\\\north\\\\LPAnorthseal'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPermissionError\u001B[0m                           Traceback (most recent call last)",
      "Input \u001B[1;32mIn [41]\u001B[0m, in \u001B[0;36m<cell line: 13>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mk=31\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03mc=0.01 \u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;124;03mmin_reads_per_cluster=2\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m##config1 = annotate_clusters('../tmp/maize14G_lpaseq.txt_31.3.gz', keyinfo)\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m config1 \u001B[38;5;241m=\u001B[39m \u001B[43mannotate_clusters\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mD:\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43mStudy\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43mSAPRC\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43mtemp\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43mnorth\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43mLPAnorthseal\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeyinfo\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m##LPA输出文件\u001B[39;00m\n\u001B[0;32m     14\u001B[0m clustering_stat(config1, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mk31min10\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Input \u001B[1;32mIn [37]\u001B[0m, in \u001B[0;36mannotate_clusters\u001B[1;34m(cluster, annotations)\u001B[0m\n\u001B[0;32m     63\u001B[0m total_clustered_reads \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     64\u001B[0m total_clustered_annotated \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 65\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mgzip\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcluster\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m IN:\n\u001B[0;32m     66\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m lines \u001B[38;5;129;01min\u001B[39;00m IN:\u001B[38;5;66;03m#.readlines():\u001B[39;00m\n\u001B[0;32m     67\u001B[0m         seq_id, group_id \u001B[38;5;241m=\u001B[39m lines\u001B[38;5;241m.\u001B[39mstrip(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\Study\\HOME\\pythone\\lib\\gzip.py:58\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(filename, mode, compresslevel, encoding, errors, newline)\u001B[0m\n\u001B[0;32m     56\u001B[0m gz_mode \u001B[38;5;241m=\u001B[39m mode\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(filename, (\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mbytes\u001B[39m, os\u001B[38;5;241m.\u001B[39mPathLike)):\n\u001B[1;32m---> 58\u001B[0m     binary_file \u001B[38;5;241m=\u001B[39m \u001B[43mGzipFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgz_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompresslevel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(filename, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(filename, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m     60\u001B[0m     binary_file \u001B[38;5;241m=\u001B[39m GzipFile(\u001B[38;5;28;01mNone\u001B[39;00m, gz_mode, compresslevel, filename)\n",
      "File \u001B[1;32mD:\\Study\\HOME\\pythone\\lib\\gzip.py:174\u001B[0m, in \u001B[0;36mGzipFile.__init__\u001B[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001B[0m\n\u001B[0;32m    172\u001B[0m     mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fileobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 174\u001B[0m     fileobj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmyfileobj \u001B[38;5;241m=\u001B[39m \u001B[43mbuiltins\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filename \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     filename \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(fileobj, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mPermissionError\u001B[0m: [Errno 13] Permission denied: 'D:\\\\Study\\\\SAPRC\\\\temp\\\\north\\\\LPAnorthseal'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "k=31\n",
    "c=0.01 \n",
    "\n",
    "min_kmer_count=2\n",
    "max_kmer_count=60000\n",
    "\n",
    "min_shared_kmers=10\n",
    "max_shared_kmers=500\n",
    "min_reads_per_cluster=2\n",
    "'''\n",
    "##config1 = annotate_clusters('../tmp/maize14G_lpaseq.txt_31.3.gz', keyinfo)\n",
    "config1 = annotate_clusters('D:\\\\Study\\\\SAPRC\\\\temp\\\\north\\\\LPAnorthseal', keyinfo)  ##LPA输出文件\n",
    "clustering_stat(config1, 'k31min10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (lizhenshi-sparc_5778)",
   "language": "python",
   "name": "pycharm-414ff09c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}