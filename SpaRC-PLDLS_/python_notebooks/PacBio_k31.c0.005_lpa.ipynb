{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"/global/u1/z/zhwang/zhong/lib/python2.7/site-packages\",\n",
    "#\"/global/u1/z/zhwang/zhong/lib/python2.7/site-packages/IPython/extensions\"\n",
    "import sys, os\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "add_libs = [\n",
    "\"D:\\Anaconda\\Lib\\site-packages\",\n",
    "\"D:\\Anaconda\\Lib\\site-packages\\IPython\\extensions\"\n",
    "]\n",
    "[sys.path.append(l) for l in add_libs]\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd  #将pandas命名为pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "%config InlineBackend.figure_format='retina'\n",
    "#定义图形细节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pacbio_key(keyfile, inputfile):\n",
    "    \n",
    "    ## read annotated transcripts  读取注释转录本\n",
    "    keyinfo = {}\n",
    "    if not(os.path.exists(keyfile) and os.path.exists(inputfile)):\n",
    "        print (\"at least one of the input files are not found.\")\n",
    "        sys.exit(0)\n",
    "    with open(keyfile, 'r') as KEY:\n",
    "        for lines in KEY:  #.readlines():\n",
    "            try:\n",
    "                anarray = lines.strip(\"\\n\").split(\"\\t\")  #strip：删除字符串中的\\n,split:将字符串使用\\t分割\n",
    "                keyinfo[anarray[0]] = anarray[4]\n",
    "            except:\n",
    "                continue\n",
    "    KEY.close()\n",
    "    #输出注释中的顶点数\n",
    "    print (\"Number of reads in the annotation: \" + '{:d}'.format(len(keyinfo.keys())))   #keys:keys() 方法返回 view 对象。这个视图对象包含列表形式的字典键。\n",
    "    known_clusters: DataFrame = pd.DataFrame.from_dict(keyinfo, 'index')  #将生成的图 生成一个字典\n",
    "    #known_counts = known_clusters.groupby([0]).size()  #通过字典中的第0个元素排序，并计算字典中的数量\n",
    "    known_counts = known_clusters.groupby(level = 0).size()  #通过字典中的第0个元素排序，并计算字典中的数量\n",
    "    #在已经注释的簇中的总reads数量\n",
    "    print (\"Total reads in annotated clusters: \" + '{:d}'.format(known_counts[known_counts>1].sum()))\n",
    "    print (\"Total annotated clusters: \" + '{:d}'.format(sum(known_counts>1)))\n",
    "    #已经注释的簇的总数\n",
    "    \n",
    "    ## annotate input reads---inputfile是seq文件\n",
    "    annotations = {}\n",
    "    no_input_reads = 0\n",
    "    no_annotated_reads = 0\n",
    "    with open(inputfile, 'r') as IN:  #以只读方式打开inputfile\n",
    "        for lines in IN:#.readlines():\n",
    "            seq_id,seq = lines.strip(\"\\n\").split(\"\\t\") #删除每一行的\\n,通过\\t分割，\\t之前的赋值给seq_id,\\t之后的赋值给seq\n",
    "            header = seq_id.split(\" \")[0] #通过“ ”分割，然后取第0个值赋值给header\n",
    "            try:\n",
    "                tid = keyinfo[header]\n",
    "                no_annotated_reads += 1\n",
    "            except:\n",
    "                tid = '-1'\n",
    "            annotations[seq_id] = tid    \n",
    "            no_input_reads +=1\n",
    "    #统计输入read总数\n",
    "    print (\"Number of reads in the input: \" + '{:d}'.format(no_input_reads))\n",
    "    print (\"Number of reads annotated in the input: \" + '{:d}'.format(no_annotated_reads))\n",
    "    #统计输入的已经有注释的reads\n",
    "    return annotations\n",
    "\n",
    "def annotate_clusters(cluster, annotations):\n",
    "    ''' \n",
    "    parse spark cluster results  解析spark聚类结果\n",
    "    cluster file format is: seqid \\t cluster_id  cluster文件格式：seqid  clusterid\n",
    "    return: [seq_name, cluster_id, annotation_source, annotation_transcript, annotation_transcript_variant]\n",
    "    \n",
    "    ''' \n",
    "    if not os.path.exists(cluster):\n",
    "        print (\"Cluster file not found.\")\n",
    "        sys.exit(0)\n",
    "        \n",
    "    results = []\n",
    "    total_clustered_reads = 0\n",
    "    total_clustered_annotated = 0\n",
    "    with open(cluster, 'r') as IN:\n",
    "        for lines in IN:#.readlines():\n",
    "            seq_id, group_id = lines.strip(\"\\n\").split(\"\\t\")\n",
    "            total_clustered_reads += 1\n",
    "            header = seq_id.split(\" \")[0]\n",
    "            if group_id > 0 and annotations[seq_id] != '-1':\n",
    "                results.append([header, group_id] + annotations[seq_id].split('.'))\n",
    "                total_clustered_annotated += 1\n",
    "                \n",
    "    IN.close()\n",
    "\n",
    "    print (\"Total reads in clusters: \" + '{:d}'.format(total_clustered_reads))\n",
    "    print (\"Total annotated reads in clusters: \" + '{:d}'.format(total_clustered_annotated))\n",
    "    return pd.DataFrame(results) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_stat(config):\n",
    "    '''\n",
    "    [seq_name, cluster_id, annotation_source, annotation_transcript, annotation_transcript_variant]\n",
    "    '''\n",
    "    ## purity，纯度\n",
    "    grouped = config[config[3] >0].groupby([1])\n",
    "    cluster_results = []\n",
    "    for name,group in grouped:\n",
    "        counts = group.groupby([3]).count()[0]\n",
    "        cluster_results.append([ len(counts), sum(counts), max(counts), int(counts[counts == max(counts)].index[0])])\n",
    "    cluster_results = np.array(cluster_results)\n",
    "    clustered_reads = sum(cluster_results[:,1])\n",
    "    print (\"Total clusters: \" + '{:d}'.format(cluster_results.shape[0]))\n",
    "    print ('{:d}'.format(clustered_reads) + \" reads are in clusters\")\n",
    "    print (\"The largest cluster has \" + '{:d}'.format(max(cluster_results[:,1]))  + \" reads.\")\n",
    "    print (\"Percent of 100% pure clusters: \" + '{:.2f}'.format(100.0* sum(cluster_results[:,0] == 1)/cluster_results.shape[0]))\n",
    "    print (\"Median purity: \" + '{:.2f}'.format(np.median(100.0* cluster_results[:,2] / cluster_results[:,1])))\n",
    "    print (\"Mean purity: \" + '{:.2f}'.format(np.mean(100.0* cluster_results[:,2] / cluster_results[:,1])))\n",
    "\n",
    "    # completeness  完整性\n",
    "    grouped = config[config[3] >0].groupby([3])\n",
    "    completeness = []\n",
    "    for name,group in grouped:\n",
    "        if len(group) <= 2:\n",
    "            continue\n",
    "        counts = group.groupby([1]).count()[0]\n",
    "        completeness.append([max(counts), sum(counts)])\n",
    "    completeness = np.array(completeness)    \n",
    "    median_completeness = np.median(100.0 * completeness[:,0] / completeness[:,1])\n",
    "    mean_completeness = np.mean(100.0 * completeness[:,0] / completeness[:,1])\n",
    "    \n",
    "    print (\"The most abundant transcript has \" + '{:d}'.format(max(completeness[:,1]))  + \" copies.\")\n",
    "    print (\"Median completeness: \" + '{:.2f}'.format(median_completeness)  )\n",
    "    print (\"Mean completeness: \" + '{:.2f}'.format(mean_completeness) )\n",
    "    \n",
    "    fig1 = plt.figure(num=1, figsize=(13, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.subplots_adjust( wspace=.2, hspace=.2 )\n",
    "\n",
    "    ax1 = fig1.add_subplot(3,3,1)\n",
    "    ax1.set_title('Clusters Size Distribution (log10) vs purity')\n",
    "    x = np.log10(cluster_results[:,1])\n",
    "    y = 100.0* cluster_results[:,2] / cluster_results[:,1]\n",
    "    sns.kdeplot(x, y, n_levels=100, shade=True, shade_lowest=True, ax=ax1);\n",
    "\n",
    "    ax2 = fig1.add_subplot(3,3,2)\n",
    "    ax2.set_title('Number of Transcripts(log2) per Cluster Distribution')\n",
    "    sns.distplot(np.log2(cluster_results[:,0]), kde=False, bins=100, ax=ax2)\n",
    "    \n",
    "    ax3 = fig1.add_subplot(3,3,3)\n",
    "    ax3.set_title('#Transcripts (log10) vs completeness')\n",
    "    x = np.log10(completeness[:,1])\n",
    "    y = 100.0 * completeness[:,0] / completeness[:,1]\n",
    "    sns.kdeplot(x, y, n_levels=100, shade=True, shade_lowest=True, ax=ax3);\n",
    "\n",
    "    # large clusters  大型集群\n",
    "    large = 1\n",
    "    x = np.log10(cluster_results[:,1])\n",
    "    sel = x>=large\n",
    "    if np.sum(sel)>10:\n",
    "        ax4 = fig1.add_subplot(3,3,4)\n",
    "        ax4.set_title('Large Clusters Size(log10) vs purity')\n",
    "        y = 100.0* cluster_results[:,2] / cluster_results[:,1]\n",
    "        sns.kdeplot(x[sel], y[sel], n_levels=10, shade=True, shade_lowest=True, ax=ax4);\n",
    "        ax4.set_xlim(left=0)\n",
    "\n",
    "        ax5 = fig1.add_subplot(3,3,5)\n",
    "        ax5.set_title('Number of Transcripts(log2) per Large Clusters')\n",
    "        ax5.set_ylim(top=2000)\n",
    "        sns.distplot(np.log2(cluster_results[:,0]), kde=False, ax=ax5)\n",
    "\n",
    "        ax6 = fig1.add_subplot(3,3,6)\n",
    "        ax6.set_title('#Transcripts (log10) vs completeness for abundant transcripts')\n",
    "        x = np.log10(completeness[:,1])\n",
    "        sel = x>=large\n",
    "        y = 100.0 * completeness[:,0] / completeness[:,1]\n",
    "        sns.kdeplot(x[sel], y[sel], n_levels=10, shade=True, shade_lowest=True, ax=ax6);\n",
    "        ax6.set_xlim(left=0)\n",
    " \n",
    "    # largest clusters  最大的集群\n",
    "    large = 2\n",
    "    x = np.log10(cluster_results[:,1])\n",
    "    sel = x>=large \n",
    "    if np.sum(sel)>1:\n",
    "        ax7 = fig1.add_subplot(3,3,7)\n",
    "        ax7.set_title('Largest Clusters Size(log10) vs purity')\n",
    "        y = 100.0* cluster_results[:,2] / cluster_results[:,1]\n",
    "        sns.regplot(x=x[sel], y=y[sel], ax=ax7);\n",
    "        ax7.set_xlim(left=0)\n",
    "        ax7.set_ylim(top=100)\n",
    "\n",
    "        ax8 = fig1.add_subplot(3,3,8)\n",
    "        ax8.set_title('Number of Transcripts(log2) per Largest Clusters')\n",
    "        ax8.set_ylim(top=200)\n",
    "        sns.distplot(np.log2(cluster_results[:,0]), kde=False, ax=ax8)\n",
    "\n",
    "        ax9 = fig1.add_subplot(3,3,9)\n",
    "        ax9.set_title('#Transcripts (log10) vs completeness for top transcripts')\n",
    "        x = np.log10(completeness[:,1])\n",
    "        sel = x>=large\n",
    "        y = 100.0 * completeness[:,0] / completeness[:,1]\n",
    "        sns.regplot(x=x[sel], y=y[sel], ax=ax9);\n",
    "        ax9.set_xlim(left=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at least one of the input files are not found.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[1;31mSystemExit\u001B[0m\u001B[1;31m:\u001B[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "inputfile = 'D:\\Study\\SAPRC\\temp\\bovine\\bovine.seq'\n",
    "keyfile = \"D:\\Study\\SAPRC\\temp\\bovine\\addseq.txt\"\n",
    "#inputfile = '../pacbio/all_samples.seq'\n",
    "#keyfile = \"../pacbio/IsoSeq_Alzheimer_2016edition_polished.promiscuous.unimapped.read_stat.txt\"\n",
    "keyinfo=get_pacbio_key(keyfile, inputfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "31-kmer reads graph, min 20 shared kmers between two reads\n",
    "'''\n",
    "config1 = annotate_clusters('../pacbio/pacbio4G_lpaseq.txt_31.min20', keyinfo)\n",
    "clustering_stat(config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "31-kmer reads graph, min 40 shared kmers between two reads\n",
    "'''\n",
    "config1 = annotate_clusters('../pacbio/pacbio4G_lpaseq.txt_31.min40', keyinfo)\n",
    "clustering_stat(config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "31-kmer reads graph, min 60 shared kmers between two reads\n",
    "'''\n",
    "config1 = annotate_clusters('../pacbio/pacbio4G_lpaseq.txt_31.min60', keyinfo)\n",
    "clustering_stat(config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-414ff09c",
   "language": "python",
   "display_name": "PyCharm (lizhenshi-sparc_5778)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}